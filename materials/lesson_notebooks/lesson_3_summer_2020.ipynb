{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Notes for Lesson 3\n",
    "\n",
    "### 1. Simple word embeddings with nltk and gensim\n",
    "\n",
    "Following: https://github.com/nltk/nltk/blob/develop/nltk/test/gensim.doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.data import find\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a couple of helper functions for cosine similarities: one deriving similarity between two words in the context of a model, the other for two vectors directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim_words(vec_model, a, b):\n",
    "    \"\"\"\n",
    "    arguments: word a, word b\n",
    "    return: cosine similarity between assoociated model vectors with a and b\n",
    "    \"\"\"\n",
    "    \n",
    "    vec_a = vec_model[a]\n",
    "    vec_b = vec_model[b]\n",
    "    \n",
    "    return np.dot(vec_a, vec_b)/np.sqrt(np.dot(vec_a, vec_a))/np.sqrt(np.dot(vec_b, vec_b))\n",
    "\n",
    "def cossim_vecs(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    arguments: word a, word b\n",
    "    return: cosine similarity between associated model vectors with a and b\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.dot(vec_a, vec_b)/np.sqrt(np.dot(vec_a, vec_a))/np.sqrt(np.dot(vec_b, vec_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK's sample word2vec embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings into a gensim model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the vocabulary? [Use model.vocab...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 43981 words in vocab.\n",
    "\n",
    "What is the **embedding**? [model['word']...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['great']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['great'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with cosine similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_words(model, 'nice', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_words(model, 'nice', 'bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now... word vectors are supposed to capture meaningful linguistic relationships. So let's try to 're-construct' the embedding vector for the word 'son' via\n",
    "\n",
    "model['son']  $\\sim$ model['boy'] - model['girl'] + model['daughter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_son = model['boy'] - model['girl'] + model['daughter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close is this constructed vector to the actual embedding bector for 'boy'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_vecs(model['son'], model_son)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close! And it is much closer to the embedding of 'boy' than other words in the family (in a double-sense): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_words(model, 'son', 'brother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_words(model, 'son', 'daughter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the approximate relationship model['son']  $\\sim$ model['boy'] - model['girl'] + model['daughter'] seems valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple BOW Classification using Word Embeddings in Keras\n",
    "\n",
    "This section roughly implements the model on slides 41 in a toy setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we know the number of words that have an embedding. Let's build the embedding matrix from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = len(model['university'])      # we know... it's 300\n",
    "\n",
    "# initialize embedding matrix and word-to-id map:\n",
    "embedding_matrix = np.zeros((len(model.vocab.keys()) + 1, EMBEDDING_DIM))       \n",
    "vocab_dict = {}\n",
    "\n",
    "# build the embedding matrix and the word-to-id map:\n",
    "for i, word in enumerate(model.vocab.keys()):\n",
    "    embedding_vector = model[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        vocab_dict[word] = i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct? Looks right.\n",
    "\n",
    "Let's build the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 5  # Keras' embedding layer expects a specific input length. Padding is often needed here.\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del tf_model\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the 'trainable=False' flag...\n",
    "\n",
    "Now let's build the model, again as a **Sequential Model**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.keras.Sequential()\n",
    "\n",
    "tf_model.add(embedding_layer)                                        # embedding layer\n",
    "tf_model.add(tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1)))    # average of embedding vectors\n",
    "tf_model.add(Dense(100, activation='relu'))                          # hidden layer\n",
    "tf_model.add(Dense(1, activation='sigmoid'))                         # classification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the dimensions of the layers?**\n",
    "\n",
    "Next: we build the model, defining input and output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether our dimension discussion was correct. Print a model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week... let's compile the model. I.e, define optimizer, loss function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.compile(optimizer='adam', loss='BinaryCrossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there... let's create some fake training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = ['this is really absolutely great', 'this is really absolutely terrible']\n",
    "train_labels = [[1], [0]]\n",
    "\n",
    "test_sentences = ['never seen anything this stupid', 'never seen anything this fantastic']\n",
    "test_labels = [[0], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then do some formatting gymnastics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_to_ids(sentences):\n",
    "    \"\"\"\n",
    "    converting a list of strings to a list of lists of word ids\n",
    "    \"\"\"\n",
    "    text_ids = []\n",
    "    for sentence in sentences:\n",
    "        example = []\n",
    "        for word in sentence.split(' '):\n",
    "            example.append(vocab_dict[word])\n",
    "        text_ids.append(example)\n",
    "\n",
    "    return  text_ids   \n",
    "\n",
    "\n",
    "train_input = np.array(sents_to_ids(train_sentences))\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_input = np.array(sents_to_ids(test_sentences))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model input are word ids in the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: let's get the start predictions. Should be random-ish. Are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_model.predict(train_input))\n",
    "print(tf_model.predict(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, looks quite random.\n",
    "\n",
    "Finally... let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.fit(train_input, train_labels, validation_data=(test_input, test_labels), epochs=1)\n",
    "tf_model.fit(train_input, train_labels, validation_data=(test_input, test_labels), epochs=150, verbose=0)\n",
    "tf_model.fit(train_input, train_labels, validation_data=(test_input, test_labels), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's good!\n",
    "\n",
    "What are train & test predictions now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yey! But we obviously cheated here with the choice of sentences. Nevertheless, the idea should be clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions for the class for joint live in-class exercises**:\n",
    "\n",
    "1) Can you relate the value for the validation loss to the prediction for the test set \n",
    "\n",
    "2) What do you think happens if you change the 'trainable' flag in the embedding layer from 'False' to 'True'?\n",
    "\n",
    "3) Let's look into the model and inspect some weights. (Use tf_model.layers. We can get weights of individual layers through  tf_model.layers[<layer_num>].weights):\n",
    "   - Related to Q2, depending on the 'trainable' flag, did the embedding matrix change?\n",
    "   \n",
    "   \n",
    "LET'S TRY IT!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
