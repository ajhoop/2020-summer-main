{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Session 9\n",
    "\n",
    "**Note:** This notebook is to be seen as a 'scratch-pad', illustrating the points made in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generation of Some Synthetic Test Data \n",
    "\n",
    "Imagine 400 train and 400 test examples with 10 features each. Each example has **two** labels, label 'a' and label 'b'. We construct these examples synthetically by picking 10 random numbers between 0 and 1, and adding for the examples with label $a =  1$ 0.2 to the first five features, and subtracting 0.2 in those features for examples with label $a = 0$. We do the same for label $b$, where we add/subtract 0.1 to features 6-10. (Obviously, this is just a toy example and the details do not matter - we just want to have some data to explore the formalisms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_X = np.array([0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_2_X = np.array([0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_3_X = np.array([-0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_4_X = np.array([-0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "\n",
    "train_X = np.concatenate([train_1_X, train_2_X, train_3_X, train_4_X])\n",
    "\n",
    "test_1_X = np.array([0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_2_X = np.array([0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_3_X = np.array([-0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_4_X = np.array([-0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "\n",
    "test_X = np.concatenate([test_1_X, test_2_X, test_3_X, test_4_X])\n",
    "\n",
    "train_1a_y = train_1b_y = train_2a_y = train_3b_y = np.reshape([[1] * 100], [100,1])\n",
    "train_2b_y = train_3a_y = train_4a_y = train_4b_y = np.reshape([[0] * 100], [100,1])\n",
    "\n",
    "test_1a_y = test_1b_y = test_2a_y = test_3b_y = np.reshape([[1] * 100], [100,1])\n",
    "test_2b_y = test_3a_y = test_4a_y = test_4b_y = np.reshape([[0] * 100], [100,1])\n",
    "\n",
    "train_a_y = np.concatenate([train_1a_y, train_2a_y, train_3a_y, train_4a_y])\n",
    "train_b_y = np.concatenate([train_1b_y, train_2b_y, train_3b_y, train_4b_y])\n",
    "\n",
    "test_a_y = np.concatenate([test_1a_y, test_2a_y, test_3a_y, test_4a_y])\n",
    "test_b_y = np.concatenate([test_1b_y, test_2b_y, test_3b_y, test_4b_y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14880381, 0.20888471, 0.37593762, 0.688705  , 0.34861701,\n",
       "        0.84755726, 0.47453406, 0.10178324, 0.48226536, 0.30355211],\n",
       "       [0.62609638, 0.59073858, 0.80794111, 0.39427203, 1.10313945,\n",
       "        0.71879424, 0.3595307 , 0.82296843, 0.31117923, 0.42294301]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_a_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a_y[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good.\n",
    "\n",
    "### 2. Keras' Functional API Formalism\n",
    "\n",
    "Keras' Functional API allows us to construct non-sequential models with multiple inputs, multiple outputs, branched models,  etc..\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "* start with definition of inputs through Input layer(s)\n",
    "* define the layers and outputs as:\n",
    "$$\\rm next\\_layer\\_activations = layer(layer\\_parameters) (previous\\_layer\\_activations)$$\n",
    "* define the model the the input(s) and output(s).  \n",
    "* 'compile' the model by specifying loss function(s), metric(s) and the optimizer.  \n",
    "\n",
    "We first set things up for one binary classification. It is most convenient to define a function called *build_model()* where we can parametrize aspects we want to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_numbers (InputLayer)   [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "classification (Dense)       (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 231\n",
      "Trainable params: 231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(activation_function='relu', \n",
    "                optimizer='adam', \n",
    "                kernel_initializer=tf.keras.initializers.he_normal(seed=1), \n",
    "                bias_initializer='zeros'):\n",
    "\n",
    "    # Define Input layer(s)\n",
    "    input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "    # Define one hidden layer and act on input\n",
    "    dense_layer = tf.keras.layers.Dense(10, \n",
    "                                        activation=activation_function, \n",
    "                                        kernel_initializer=kernel_initializer, \n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        name='dense') # layer definition\n",
    "    dense_output = dense_layer(input_numbers)  # layer acting on previous layer's output\n",
    "    \n",
    "    # Define one hidden layer and act on input\n",
    "    dense_layer_2 = tf.keras.layers.Dense(10, \n",
    "                                        activation=activation_function, \n",
    "                                        kernel_initializer=kernel_initializer, \n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        name='dense_2') # layer definition\n",
    "    dense_output_2 = dense_layer_2(dense_output)  # layer acting on previous layer's output\n",
    "\n",
    "    \n",
    "\n",
    "    # Define classification layer and act on previous output to classify examples\n",
    "    classification_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='classification') # layer definition\n",
    "    classification_output = classification_layer(dense_output_2)   # layer acting on previous layer's output\n",
    "\n",
    "    # Build and compile model\n",
    "    model = tf.keras.models.Model(input_numbers, classification_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy!   \n",
    "\n",
    "**Question:** Do the dimensions look right? \n",
    "\n",
    "### 3.  Training Optimizations\n",
    "\n",
    "Great, let's do some experimenting. We run the model above for the synthetic data set we defined, only considering the first label, and look at a few variations. Specifically, let's play with:\n",
    "\n",
    "* the **activation function** in the hidden layer     \n",
    "* the **initialization**    \n",
    "* the **optimizer**\n",
    "\n",
    "We run everything for 40 epochs and compare the results. (We split the training into two parts to cut down on reporting. 'verbose=0' supresses output. But we still want to see the final loss so we add one epoch with 'verbose=1'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.2598 - val_loss: 0.2270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90c010ecd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = build_model()  # This one should be good...\n",
    "\n",
    "model_1.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_1.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our supposed best setup, and it defines our baselines. \n",
    "\n",
    "Let's veer off and see how loss, etc. change. First, we use $tanh()$ as opposed to $relu()$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.2664 - val_loss: 0.2296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90b96f9650>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = build_model(activation_function='tanh')  \n",
    "\n",
    "model_2.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_2.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit worse. [Of course, the initialization is random, so things may be different on future runs.]\n",
    "\n",
    "Next, we change the optimizer to stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.4956 - val_loss: 0.4756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90bb696750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = build_model(optimizer='sgd')  \n",
    "\n",
    "model_3.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_3.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot worse!!\n",
    "\n",
    "Going back to the activation function, what about $sigmoid()$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 0.5621 - val_loss: 0.5476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90bb5aacd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = build_model(activation_function='sigmoid')  \n",
    "\n",
    "model_4.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_4.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good!\n",
    "\n",
    "Lastly, what about choosing general random matrix initialization as opposed to He/Xavier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 88us/sample - loss: 0.2325 - val_loss: 0.1902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90c17b7890>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = build_model(kernel_initializer='random_normal')  \n",
    "\n",
    "model_5.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_5.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit worse too.\n",
    "\n",
    "**Critical Note:** We were cheating a bit here! There is a stochastic component and if one re-reruns the cells results will differ. In fact, *in this simple case* the last configuration is often the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting Models - Multiple Outputs\n",
    "\n",
    "Now, let's go back to our dataset and let's try to predict two labels. We want to model that by *splitting* the model, i.e., two distinct hidden layers act on the input, and we use those two branches to predict the two labels:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = tf.keras.layers.Dense(10, activation='relu', name='dense_1')\n",
    "dense_layer_2 = tf.keras.layers.Dense(10, activation='relu', name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)     # hideen layer 1 acting on input\n",
    "dense_output_2 = dense_layer_2(input_numbers)     # hideen layer 2 acting on input\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')  # classification layer 1 for first branch\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')  # classification layer 2 for second branch\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model = tf.keras.models.Model(model_input, model_output)\n",
    "model.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_numbers (InputLayer)      [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           110         input_numbers[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           110         input_numbers[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "classification_1 (Dense)        (None, 1)            11          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classification_2 (Dense)        (None, 1)            11          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 242\n",
      "Trainable params: 242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "Epoch 1/150\n",
      "400/400 [==============================] - 1s 2ms/sample - loss: 1.5062 - classification_1_loss: 0.8096 - classification_2_loss: 0.6986 - classification_1_acc: 0.5000 - classification_2_acc: 0.5525 - val_loss: 1.4823 - val_classification_1_loss: 0.7621 - val_classification_2_loss: 0.7076 - val_classification_1_acc: 0.5000 - val_classification_2_acc: 0.5475\n",
      "Epoch 2/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 1.4565 - classification_1_loss: 0.7759 - classification_2_loss: 0.6774 - classification_1_acc: 0.5000 - classification_2_acc: 0.5500 - val_loss: 1.4425 - val_classification_1_loss: 0.7403 - val_classification_2_loss: 0.6941 - val_classification_1_acc: 0.5000 - val_classification_2_acc: 0.5375\n",
      "Epoch 3/150\n",
      "400/400 [==============================] - 0s 99us/sample - loss: 1.4221 - classification_1_loss: 0.7542 - classification_2_loss: 0.6662 - classification_1_acc: 0.5000 - classification_2_acc: 0.5750 - val_loss: 1.4146 - val_classification_1_loss: 0.7222 - val_classification_2_loss: 0.6876 - val_classification_1_acc: 0.5000 - val_classification_2_acc: 0.5600\n",
      "Epoch 4/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 1.3956 - classification_1_loss: 0.7354 - classification_2_loss: 0.6611 - classification_1_acc: 0.4975 - classification_2_acc: 0.5875 - val_loss: 1.3923 - val_classification_1_loss: 0.7060 - val_classification_2_loss: 0.6838 - val_classification_1_acc: 0.4950 - val_classification_2_acc: 0.5750\n",
      "Epoch 5/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 1.3749 - classification_1_loss: 0.7200 - classification_2_loss: 0.6551 - classification_1_acc: 0.4900 - classification_2_acc: 0.5950 - val_loss: 1.3715 - val_classification_1_loss: 0.6898 - val_classification_2_loss: 0.6809 - val_classification_1_acc: 0.5025 - val_classification_2_acc: 0.5825\n",
      "Epoch 6/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 1.3547 - classification_1_loss: 0.6997 - classification_2_loss: 0.6571 - classification_1_acc: 0.4875 - classification_2_acc: 0.6075 - val_loss: 1.3521 - val_classification_1_loss: 0.6745 - val_classification_2_loss: 0.6783 - val_classification_1_acc: 0.5125 - val_classification_2_acc: 0.5850\n",
      "Epoch 7/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 1.3361 - classification_1_loss: 0.6815 - classification_2_loss: 0.6509 - classification_1_acc: 0.5025 - classification_2_acc: 0.6075 - val_loss: 1.3330 - val_classification_1_loss: 0.6595 - val_classification_2_loss: 0.6757 - val_classification_1_acc: 0.5575 - val_classification_2_acc: 0.5950\n",
      "Epoch 8/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 1.3176 - classification_1_loss: 0.6673 - classification_2_loss: 0.6466 - classification_1_acc: 0.5525 - classification_2_acc: 0.6275 - val_loss: 1.3149 - val_classification_1_loss: 0.6450 - val_classification_2_loss: 0.6732 - val_classification_1_acc: 0.6000 - val_classification_2_acc: 0.6050\n",
      "Epoch 9/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 1.3000 - classification_1_loss: 0.6545 - classification_2_loss: 0.6471 - classification_1_acc: 0.5975 - classification_2_acc: 0.6350 - val_loss: 1.2971 - val_classification_1_loss: 0.6306 - val_classification_2_loss: 0.6709 - val_classification_1_acc: 0.6925 - val_classification_2_acc: 0.5975\n",
      "Epoch 10/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 1.2828 - classification_1_loss: 0.6396 - classification_2_loss: 0.6412 - classification_1_acc: 0.6650 - classification_2_acc: 0.6425 - val_loss: 1.2792 - val_classification_1_loss: 0.6158 - val_classification_2_loss: 0.6686 - val_classification_1_acc: 0.7725 - val_classification_2_acc: 0.6000\n",
      "Epoch 11/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 1.2662 - classification_1_loss: 0.6250 - classification_2_loss: 0.6419 - classification_1_acc: 0.7175 - classification_2_acc: 0.6550 - val_loss: 1.2616 - val_classification_1_loss: 0.6011 - val_classification_2_loss: 0.6664 - val_classification_1_acc: 0.8025 - val_classification_2_acc: 0.5975\n",
      "Epoch 12/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 1.2492 - classification_1_loss: 0.6102 - classification_2_loss: 0.6360 - classification_1_acc: 0.7550 - classification_2_acc: 0.6625 - val_loss: 1.2444 - val_classification_1_loss: 0.5868 - val_classification_2_loss: 0.6641 - val_classification_1_acc: 0.8125 - val_classification_2_acc: 0.6000\n",
      "Epoch 13/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 1.2331 - classification_1_loss: 0.5972 - classification_2_loss: 0.6302 - classification_1_acc: 0.7600 - classification_2_acc: 0.6650 - val_loss: 1.2275 - val_classification_1_loss: 0.5724 - val_classification_2_loss: 0.6618 - val_classification_1_acc: 0.8275 - val_classification_2_acc: 0.6000\n",
      "Epoch 14/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 1.2178 - classification_1_loss: 0.5867 - classification_2_loss: 0.6323 - classification_1_acc: 0.7700 - classification_2_acc: 0.6625 - val_loss: 1.2106 - val_classification_1_loss: 0.5584 - val_classification_2_loss: 0.6597 - val_classification_1_acc: 0.8350 - val_classification_2_acc: 0.6075\n",
      "Epoch 15/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 1.2020 - classification_1_loss: 0.5696 - classification_2_loss: 0.6301 - classification_1_acc: 0.7700 - classification_2_acc: 0.6625 - val_loss: 1.1946 - val_classification_1_loss: 0.5448 - val_classification_2_loss: 0.6574 - val_classification_1_acc: 0.8375 - val_classification_2_acc: 0.6200\n",
      "Epoch 16/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 1.1877 - classification_1_loss: 0.5614 - classification_2_loss: 0.6270 - classification_1_acc: 0.7775 - classification_2_acc: 0.6675 - val_loss: 1.1783 - val_classification_1_loss: 0.5307 - val_classification_2_loss: 0.6553 - val_classification_1_acc: 0.8475 - val_classification_2_acc: 0.6175\n",
      "Epoch 17/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 1.1720 - classification_1_loss: 0.5460 - classification_2_loss: 0.6257 - classification_1_acc: 0.7725 - classification_2_acc: 0.6625 - val_loss: 1.1629 - val_classification_1_loss: 0.5181 - val_classification_2_loss: 0.6531 - val_classification_1_acc: 0.8400 - val_classification_2_acc: 0.6250\n",
      "Epoch 18/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 1.1580 - classification_1_loss: 0.5352 - classification_2_loss: 0.6245 - classification_1_acc: 0.7750 - classification_2_acc: 0.6625 - val_loss: 1.1475 - val_classification_1_loss: 0.5052 - val_classification_2_loss: 0.6510 - val_classification_1_acc: 0.8450 - val_classification_2_acc: 0.6300\n",
      "Epoch 19/150\n",
      "400/400 [==============================] - 0s 120us/sample - loss: 1.1435 - classification_1_loss: 0.5280 - classification_2_loss: 0.6190 - classification_1_acc: 0.7750 - classification_2_acc: 0.6625 - val_loss: 1.1328 - val_classification_1_loss: 0.4928 - val_classification_2_loss: 0.6488 - val_classification_1_acc: 0.8500 - val_classification_2_acc: 0.6250\n",
      "Epoch 20/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 1.1298 - classification_1_loss: 0.5098 - classification_2_loss: 0.6168 - classification_1_acc: 0.7850 - classification_2_acc: 0.6800 - val_loss: 1.1178 - val_classification_1_loss: 0.4799 - val_classification_2_loss: 0.6466 - val_classification_1_acc: 0.8550 - val_classification_2_acc: 0.6225\n",
      "Epoch 21/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 1.1162 - classification_1_loss: 0.5001 - classification_2_loss: 0.6138 - classification_1_acc: 0.7875 - classification_2_acc: 0.6900 - val_loss: 1.1036 - val_classification_1_loss: 0.4689 - val_classification_2_loss: 0.6443 - val_classification_1_acc: 0.8500 - val_classification_2_acc: 0.6300\n",
      "Epoch 22/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 1.1022 - classification_1_loss: 0.4889 - classification_2_loss: 0.6076 - classification_1_acc: 0.7975 - classification_2_acc: 0.6900 - val_loss: 1.0892 - val_classification_1_loss: 0.4567 - val_classification_2_loss: 0.6417 - val_classification_1_acc: 0.8550 - val_classification_2_acc: 0.6425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 1.0889 - classification_1_loss: 0.4769 - classification_2_loss: 0.6086 - classification_1_acc: 0.8025 - classification_2_acc: 0.6925 - val_loss: 1.0760 - val_classification_1_loss: 0.4464 - val_classification_2_loss: 0.6393 - val_classification_1_acc: 0.8650 - val_classification_2_acc: 0.6400\n",
      "Epoch 24/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 1.0756 - classification_1_loss: 0.4682 - classification_2_loss: 0.6078 - classification_1_acc: 0.8100 - classification_2_acc: 0.7000 - val_loss: 1.0618 - val_classification_1_loss: 0.4349 - val_classification_2_loss: 0.6366 - val_classification_1_acc: 0.8625 - val_classification_2_acc: 0.6375\n",
      "Epoch 25/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 1.0628 - classification_1_loss: 0.4591 - classification_2_loss: 0.6022 - classification_1_acc: 0.8150 - classification_2_acc: 0.7000 - val_loss: 1.0488 - val_classification_1_loss: 0.4245 - val_classification_2_loss: 0.6339 - val_classification_1_acc: 0.8650 - val_classification_2_acc: 0.6400\n",
      "Epoch 26/150\n",
      "400/400 [==============================] - 0s 117us/sample - loss: 1.0508 - classification_1_loss: 0.4497 - classification_2_loss: 0.5992 - classification_1_acc: 0.8200 - classification_2_acc: 0.7050 - val_loss: 1.0362 - val_classification_1_loss: 0.4149 - val_classification_2_loss: 0.6312 - val_classification_1_acc: 0.8675 - val_classification_2_acc: 0.6400\n",
      "Epoch 27/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 1.0401 - classification_1_loss: 0.4423 - classification_2_loss: 0.5943 - classification_1_acc: 0.8175 - classification_2_acc: 0.6925 - val_loss: 1.0246 - val_classification_1_loss: 0.4058 - val_classification_2_loss: 0.6284 - val_classification_1_acc: 0.8725 - val_classification_2_acc: 0.6425\n",
      "Epoch 28/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 1.0280 - classification_1_loss: 0.4342 - classification_2_loss: 0.5916 - classification_1_acc: 0.8250 - classification_2_acc: 0.6925 - val_loss: 1.0131 - val_classification_1_loss: 0.3969 - val_classification_2_loss: 0.6259 - val_classification_1_acc: 0.8750 - val_classification_2_acc: 0.6400\n",
      "Epoch 29/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 1.0176 - classification_1_loss: 0.4325 - classification_2_loss: 0.5880 - classification_1_acc: 0.8275 - classification_2_acc: 0.7000 - val_loss: 1.0021 - val_classification_1_loss: 0.3887 - val_classification_2_loss: 0.6236 - val_classification_1_acc: 0.8750 - val_classification_2_acc: 0.6450\n",
      "Epoch 30/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 1.0072 - classification_1_loss: 0.4201 - classification_2_loss: 0.5871 - classification_1_acc: 0.8325 - classification_2_acc: 0.7100 - val_loss: 0.9920 - val_classification_1_loss: 0.3813 - val_classification_2_loss: 0.6211 - val_classification_1_acc: 0.8750 - val_classification_2_acc: 0.6500\n",
      "Epoch 31/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.9975 - classification_1_loss: 0.4140 - classification_2_loss: 0.5852 - classification_1_acc: 0.8350 - classification_2_acc: 0.7150 - val_loss: 0.9820 - val_classification_1_loss: 0.3735 - val_classification_2_loss: 0.6186 - val_classification_1_acc: 0.8750 - val_classification_2_acc: 0.6550\n",
      "Epoch 32/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.9882 - classification_1_loss: 0.4111 - classification_2_loss: 0.5805 - classification_1_acc: 0.8350 - classification_2_acc: 0.6925 - val_loss: 0.9733 - val_classification_1_loss: 0.3664 - val_classification_2_loss: 0.6163 - val_classification_1_acc: 0.8775 - val_classification_2_acc: 0.6550\n",
      "Epoch 33/150\n",
      "400/400 [==============================] - 0s 115us/sample - loss: 0.9790 - classification_1_loss: 0.4064 - classification_2_loss: 0.5809 - classification_1_acc: 0.8350 - classification_2_acc: 0.6950 - val_loss: 0.9642 - val_classification_1_loss: 0.3596 - val_classification_2_loss: 0.6140 - val_classification_1_acc: 0.8850 - val_classification_2_acc: 0.6525\n",
      "Epoch 34/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.9703 - classification_1_loss: 0.3968 - classification_2_loss: 0.5769 - classification_1_acc: 0.8350 - classification_2_acc: 0.6975 - val_loss: 0.9557 - val_classification_1_loss: 0.3533 - val_classification_2_loss: 0.6116 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.6600\n",
      "Epoch 35/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.9615 - classification_1_loss: 0.3907 - classification_2_loss: 0.5729 - classification_1_acc: 0.8375 - classification_2_acc: 0.6950 - val_loss: 0.9478 - val_classification_1_loss: 0.3473 - val_classification_2_loss: 0.6093 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6650\n",
      "Epoch 36/150\n",
      "400/400 [==============================] - 0s 109us/sample - loss: 0.9533 - classification_1_loss: 0.3796 - classification_2_loss: 0.5709 - classification_1_acc: 0.8325 - classification_2_acc: 0.6925 - val_loss: 0.9400 - val_classification_1_loss: 0.3420 - val_classification_2_loss: 0.6072 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6675\n",
      "Epoch 37/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.9452 - classification_1_loss: 0.3731 - classification_2_loss: 0.5675 - classification_1_acc: 0.8350 - classification_2_acc: 0.7025 - val_loss: 0.9314 - val_classification_1_loss: 0.3366 - val_classification_2_loss: 0.6045 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6725\n",
      "Epoch 38/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.9381 - classification_1_loss: 0.3694 - classification_2_loss: 0.5656 - classification_1_acc: 0.8350 - classification_2_acc: 0.7025 - val_loss: 0.9238 - val_classification_1_loss: 0.3316 - val_classification_2_loss: 0.6022 - val_classification_1_acc: 0.8850 - val_classification_2_acc: 0.6775\n",
      "Epoch 39/150\n",
      "400/400 [==============================] - 0s 111us/sample - loss: 0.9304 - classification_1_loss: 0.3674 - classification_2_loss: 0.5612 - classification_1_acc: 0.8325 - classification_2_acc: 0.7025 - val_loss: 0.9164 - val_classification_1_loss: 0.3261 - val_classification_2_loss: 0.6000 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.6775\n",
      "Epoch 40/150\n",
      "400/400 [==============================] - 0s 110us/sample - loss: 0.9232 - classification_1_loss: 0.3623 - classification_2_loss: 0.5651 - classification_1_acc: 0.8325 - classification_2_acc: 0.7025 - val_loss: 0.9101 - val_classification_1_loss: 0.3218 - val_classification_2_loss: 0.5979 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.6850\n",
      "Epoch 41/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.9158 - classification_1_loss: 0.3547 - classification_2_loss: 0.5548 - classification_1_acc: 0.8325 - classification_2_acc: 0.7025 - val_loss: 0.9034 - val_classification_1_loss: 0.3174 - val_classification_2_loss: 0.5956 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.6825\n",
      "Epoch 42/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.9100 - classification_1_loss: 0.3603 - classification_2_loss: 0.5545 - classification_1_acc: 0.8350 - classification_2_acc: 0.7050 - val_loss: 0.8974 - val_classification_1_loss: 0.3132 - val_classification_2_loss: 0.5936 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.6825\n",
      "Epoch 43/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.9036 - classification_1_loss: 0.3486 - classification_2_loss: 0.5517 - classification_1_acc: 0.8350 - classification_2_acc: 0.7050 - val_loss: 0.8910 - val_classification_1_loss: 0.3097 - val_classification_2_loss: 0.5911 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.6825\n",
      "Epoch 44/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.8971 - classification_1_loss: 0.3438 - classification_2_loss: 0.5511 - classification_1_acc: 0.8400 - classification_2_acc: 0.7050 - val_loss: 0.8844 - val_classification_1_loss: 0.3050 - val_classification_2_loss: 0.5888 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.8916 - classification_1_loss: 0.3430 - classification_2_loss: 0.5504 - classification_1_acc: 0.8450 - classification_2_acc: 0.7125 - val_loss: 0.8785 - val_classification_1_loss: 0.3013 - val_classification_2_loss: 0.5865 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.6850\n",
      "Epoch 46/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.8850 - classification_1_loss: 0.3360 - classification_2_loss: 0.5505 - classification_1_acc: 0.8450 - classification_2_acc: 0.7100 - val_loss: 0.8732 - val_classification_1_loss: 0.2981 - val_classification_2_loss: 0.5844 - val_classification_1_acc: 0.9025 - val_classification_2_acc: 0.6850\n",
      "Epoch 47/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.8799 - classification_1_loss: 0.3314 - classification_2_loss: 0.5446 - classification_1_acc: 0.8450 - classification_2_acc: 0.7100 - val_loss: 0.8677 - val_classification_1_loss: 0.2948 - val_classification_2_loss: 0.5822 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.6850\n",
      "Epoch 48/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.8744 - classification_1_loss: 0.3319 - classification_2_loss: 0.5408 - classification_1_acc: 0.8450 - classification_2_acc: 0.7175 - val_loss: 0.8620 - val_classification_1_loss: 0.2913 - val_classification_2_loss: 0.5798 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.6875\n",
      "Epoch 49/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.8685 - classification_1_loss: 0.3342 - classification_2_loss: 0.5415 - classification_1_acc: 0.8500 - classification_2_acc: 0.7150 - val_loss: 0.8570 - val_classification_1_loss: 0.2881 - val_classification_2_loss: 0.5778 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.6925\n",
      "Epoch 50/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.8631 - classification_1_loss: 0.3229 - classification_2_loss: 0.5397 - classification_1_acc: 0.8575 - classification_2_acc: 0.7175 - val_loss: 0.8518 - val_classification_1_loss: 0.2849 - val_classification_2_loss: 0.5753 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.6925\n",
      "Epoch 51/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.8590 - classification_1_loss: 0.3237 - classification_2_loss: 0.5324 - classification_1_acc: 0.8525 - classification_2_acc: 0.7225 - val_loss: 0.8458 - val_classification_1_loss: 0.2818 - val_classification_2_loss: 0.5726 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.6900\n",
      "Epoch 52/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.8531 - classification_1_loss: 0.3190 - classification_2_loss: 0.5369 - classification_1_acc: 0.8525 - classification_2_acc: 0.7250 - val_loss: 0.8414 - val_classification_1_loss: 0.2792 - val_classification_2_loss: 0.5706 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7000\n",
      "Epoch 53/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.8499 - classification_1_loss: 0.3120 - classification_2_loss: 0.5289 - classification_1_acc: 0.8550 - classification_2_acc: 0.7200 - val_loss: 0.8374 - val_classification_1_loss: 0.2769 - val_classification_2_loss: 0.5688 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7025\n",
      "Epoch 54/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 0.8430 - classification_1_loss: 0.3061 - classification_2_loss: 0.5322 - classification_1_acc: 0.8550 - classification_2_acc: 0.7300 - val_loss: 0.8319 - val_classification_1_loss: 0.2740 - val_classification_2_loss: 0.5664 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.6950\n",
      "Epoch 55/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.8389 - classification_1_loss: 0.3097 - classification_2_loss: 0.5278 - classification_1_acc: 0.8550 - classification_2_acc: 0.7350 - val_loss: 0.8268 - val_classification_1_loss: 0.2710 - val_classification_2_loss: 0.5642 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.6950\n",
      "Epoch 56/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.8349 - classification_1_loss: 0.3105 - classification_2_loss: 0.5245 - classification_1_acc: 0.8575 - classification_2_acc: 0.7350 - val_loss: 0.8224 - val_classification_1_loss: 0.2685 - val_classification_2_loss: 0.5620 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.6975\n",
      "Epoch 57/150\n",
      "400/400 [==============================] - 0s 114us/sample - loss: 0.8299 - classification_1_loss: 0.3084 - classification_2_loss: 0.5237 - classification_1_acc: 0.8550 - classification_2_acc: 0.7475 - val_loss: 0.8177 - val_classification_1_loss: 0.2664 - val_classification_2_loss: 0.5598 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.6975\n",
      "Epoch 58/150\n",
      "400/400 [==============================] - 0s 114us/sample - loss: 0.8252 - classification_1_loss: 0.3053 - classification_2_loss: 0.5230 - classification_1_acc: 0.8525 - classification_2_acc: 0.7450 - val_loss: 0.8145 - val_classification_1_loss: 0.2649 - val_classification_2_loss: 0.5580 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.6975\n",
      "Epoch 59/150\n",
      "400/400 [==============================] - 0s 113us/sample - loss: 0.8215 - classification_1_loss: 0.3044 - classification_2_loss: 0.5151 - classification_1_acc: 0.8625 - classification_2_acc: 0.7525 - val_loss: 0.8100 - val_classification_1_loss: 0.2622 - val_classification_2_loss: 0.5561 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7000\n",
      "Epoch 60/150\n",
      "400/400 [==============================] - 0s 114us/sample - loss: 0.8178 - classification_1_loss: 0.3039 - classification_2_loss: 0.5163 - classification_1_acc: 0.8650 - classification_2_acc: 0.7400 - val_loss: 0.8072 - val_classification_1_loss: 0.2598 - val_classification_2_loss: 0.5546 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7100\n",
      "Epoch 61/150\n",
      "400/400 [==============================] - 0s 119us/sample - loss: 0.8129 - classification_1_loss: 0.2956 - classification_2_loss: 0.5153 - classification_1_acc: 0.8675 - classification_2_acc: 0.7450 - val_loss: 0.8022 - val_classification_1_loss: 0.2575 - val_classification_2_loss: 0.5523 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7050\n",
      "Epoch 62/150\n",
      "400/400 [==============================] - 0s 110us/sample - loss: 0.8090 - classification_1_loss: 0.2963 - classification_2_loss: 0.5112 - classification_1_acc: 0.8725 - classification_2_acc: 0.7475 - val_loss: 0.7986 - val_classification_1_loss: 0.2552 - val_classification_2_loss: 0.5503 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7075\n",
      "Epoch 63/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.8052 - classification_1_loss: 0.2922 - classification_2_loss: 0.5088 - classification_1_acc: 0.8725 - classification_2_acc: 0.7475 - val_loss: 0.7950 - val_classification_1_loss: 0.2533 - val_classification_2_loss: 0.5484 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7125\n",
      "Epoch 64/150\n",
      "400/400 [==============================] - 0s 145us/sample - loss: 0.8015 - classification_1_loss: 0.2950 - classification_2_loss: 0.5136 - classification_1_acc: 0.8750 - classification_2_acc: 0.7525 - val_loss: 0.7911 - val_classification_1_loss: 0.2519 - val_classification_2_loss: 0.5464 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7125\n",
      "Epoch 65/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.7977 - classification_1_loss: 0.2831 - classification_2_loss: 0.5095 - classification_1_acc: 0.8775 - classification_2_acc: 0.7500 - val_loss: 0.7878 - val_classification_1_loss: 0.2495 - val_classification_2_loss: 0.5446 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7150\n",
      "Epoch 66/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7938 - classification_1_loss: 0.2879 - classification_2_loss: 0.5080 - classification_1_acc: 0.8750 - classification_2_acc: 0.7500 - val_loss: 0.7844 - val_classification_1_loss: 0.2477 - val_classification_2_loss: 0.5428 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.7902 - classification_1_loss: 0.2879 - classification_2_loss: 0.5094 - classification_1_acc: 0.8800 - classification_2_acc: 0.7550 - val_loss: 0.7801 - val_classification_1_loss: 0.2462 - val_classification_2_loss: 0.5410 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7200\n",
      "Epoch 68/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.7869 - classification_1_loss: 0.2884 - classification_2_loss: 0.5038 - classification_1_acc: 0.8775 - classification_2_acc: 0.7575 - val_loss: 0.7771 - val_classification_1_loss: 0.2446 - val_classification_2_loss: 0.5390 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7200\n",
      "Epoch 69/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.7832 - classification_1_loss: 0.2776 - classification_2_loss: 0.4996 - classification_1_acc: 0.8825 - classification_2_acc: 0.7575 - val_loss: 0.7739 - val_classification_1_loss: 0.2429 - val_classification_2_loss: 0.5372 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7150\n",
      "Epoch 70/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.7806 - classification_1_loss: 0.2751 - classification_2_loss: 0.4989 - classification_1_acc: 0.8850 - classification_2_acc: 0.7575 - val_loss: 0.7718 - val_classification_1_loss: 0.2413 - val_classification_2_loss: 0.5361 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7275\n",
      "Epoch 71/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7773 - classification_1_loss: 0.2746 - classification_2_loss: 0.5072 - classification_1_acc: 0.8850 - classification_2_acc: 0.7575 - val_loss: 0.7678 - val_classification_1_loss: 0.2396 - val_classification_2_loss: 0.5341 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7250\n",
      "Epoch 72/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.7744 - classification_1_loss: 0.2772 - classification_2_loss: 0.4983 - classification_1_acc: 0.8800 - classification_2_acc: 0.7600 - val_loss: 0.7649 - val_classification_1_loss: 0.2376 - val_classification_2_loss: 0.5325 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7275\n",
      "Epoch 73/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7714 - classification_1_loss: 0.2786 - classification_2_loss: 0.4935 - classification_1_acc: 0.8800 - classification_2_acc: 0.7625 - val_loss: 0.7615 - val_classification_1_loss: 0.2361 - val_classification_2_loss: 0.5310 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7275\n",
      "Epoch 74/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7690 - classification_1_loss: 0.2756 - classification_2_loss: 0.4947 - classification_1_acc: 0.8875 - classification_2_acc: 0.7575 - val_loss: 0.7589 - val_classification_1_loss: 0.2355 - val_classification_2_loss: 0.5294 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7325\n",
      "Epoch 75/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7658 - classification_1_loss: 0.2791 - classification_2_loss: 0.4981 - classification_1_acc: 0.8875 - classification_2_acc: 0.7625 - val_loss: 0.7562 - val_classification_1_loss: 0.2335 - val_classification_2_loss: 0.5281 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7325\n",
      "Epoch 76/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7620 - classification_1_loss: 0.2663 - classification_2_loss: 0.4950 - classification_1_acc: 0.8875 - classification_2_acc: 0.7625 - val_loss: 0.7547 - val_classification_1_loss: 0.2329 - val_classification_2_loss: 0.5269 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7325\n",
      "Epoch 77/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7592 - classification_1_loss: 0.2661 - classification_2_loss: 0.4897 - classification_1_acc: 0.8875 - classification_2_acc: 0.7675 - val_loss: 0.7509 - val_classification_1_loss: 0.2310 - val_classification_2_loss: 0.5251 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7400\n",
      "Epoch 78/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7564 - classification_1_loss: 0.2659 - classification_2_loss: 0.4894 - classification_1_acc: 0.8875 - classification_2_acc: 0.7625 - val_loss: 0.7480 - val_classification_1_loss: 0.2295 - val_classification_2_loss: 0.5237 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7400\n",
      "Epoch 79/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7537 - classification_1_loss: 0.2647 - classification_2_loss: 0.4920 - classification_1_acc: 0.8875 - classification_2_acc: 0.7625 - val_loss: 0.7461 - val_classification_1_loss: 0.2283 - val_classification_2_loss: 0.5226 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7325\n",
      "Epoch 80/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7508 - classification_1_loss: 0.2638 - classification_2_loss: 0.4830 - classification_1_acc: 0.8875 - classification_2_acc: 0.7750 - val_loss: 0.7439 - val_classification_1_loss: 0.2271 - val_classification_2_loss: 0.5212 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7325\n",
      "Epoch 81/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7485 - classification_1_loss: 0.2579 - classification_2_loss: 0.4837 - classification_1_acc: 0.8850 - classification_2_acc: 0.7725 - val_loss: 0.7416 - val_classification_1_loss: 0.2255 - val_classification_2_loss: 0.5200 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7300\n",
      "Epoch 82/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.7469 - classification_1_loss: 0.2617 - classification_2_loss: 0.4841 - classification_1_acc: 0.8875 - classification_2_acc: 0.7675 - val_loss: 0.7386 - val_classification_1_loss: 0.2249 - val_classification_2_loss: 0.5186 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7400\n",
      "Epoch 83/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.7432 - classification_1_loss: 0.2638 - classification_2_loss: 0.4838 - classification_1_acc: 0.8850 - classification_2_acc: 0.7675 - val_loss: 0.7359 - val_classification_1_loss: 0.2231 - val_classification_2_loss: 0.5171 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7400\n",
      "Epoch 84/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7406 - classification_1_loss: 0.2632 - classification_2_loss: 0.4786 - classification_1_acc: 0.8850 - classification_2_acc: 0.7700 - val_loss: 0.7339 - val_classification_1_loss: 0.2221 - val_classification_2_loss: 0.5159 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7400\n",
      "Epoch 85/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7386 - classification_1_loss: 0.2596 - classification_2_loss: 0.4746 - classification_1_acc: 0.8875 - classification_2_acc: 0.7775 - val_loss: 0.7330 - val_classification_1_loss: 0.2213 - val_classification_2_loss: 0.5151 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7450\n",
      "Epoch 86/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.7370 - classification_1_loss: 0.2633 - classification_2_loss: 0.4823 - classification_1_acc: 0.8850 - classification_2_acc: 0.7800 - val_loss: 0.7289 - val_classification_1_loss: 0.2197 - val_classification_2_loss: 0.5132 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7450\n",
      "Epoch 87/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7344 - classification_1_loss: 0.2541 - classification_2_loss: 0.4744 - classification_1_acc: 0.8850 - classification_2_acc: 0.7825 - val_loss: 0.7282 - val_classification_1_loss: 0.2182 - val_classification_2_loss: 0.5128 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7500\n",
      "Epoch 88/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.7313 - classification_1_loss: 0.2548 - classification_2_loss: 0.4804 - classification_1_acc: 0.8850 - classification_2_acc: 0.7825 - val_loss: 0.7253 - val_classification_1_loss: 0.2172 - val_classification_2_loss: 0.5113 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7294 - classification_1_loss: 0.2560 - classification_2_loss: 0.4715 - classification_1_acc: 0.8850 - classification_2_acc: 0.7825 - val_loss: 0.7228 - val_classification_1_loss: 0.2163 - val_classification_2_loss: 0.5099 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7500\n",
      "Epoch 90/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 0.7272 - classification_1_loss: 0.2613 - classification_2_loss: 0.4706 - classification_1_acc: 0.8875 - classification_2_acc: 0.7775 - val_loss: 0.7214 - val_classification_1_loss: 0.2163 - val_classification_2_loss: 0.5088 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7525\n",
      "Epoch 91/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 0.7250 - classification_1_loss: 0.2538 - classification_2_loss: 0.4730 - classification_1_acc: 0.8875 - classification_2_acc: 0.7775 - val_loss: 0.7187 - val_classification_1_loss: 0.2147 - val_classification_2_loss: 0.5075 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7525\n",
      "Epoch 92/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7229 - classification_1_loss: 0.2486 - classification_2_loss: 0.4721 - classification_1_acc: 0.8850 - classification_2_acc: 0.7825 - val_loss: 0.7173 - val_classification_1_loss: 0.2134 - val_classification_2_loss: 0.5068 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7550\n",
      "Epoch 93/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7208 - classification_1_loss: 0.2616 - classification_2_loss: 0.4701 - classification_1_acc: 0.8875 - classification_2_acc: 0.7850 - val_loss: 0.7159 - val_classification_1_loss: 0.2132 - val_classification_2_loss: 0.5058 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7575\n",
      "Epoch 94/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7187 - classification_1_loss: 0.2481 - classification_2_loss: 0.4707 - classification_1_acc: 0.8875 - classification_2_acc: 0.7825 - val_loss: 0.7136 - val_classification_1_loss: 0.2117 - val_classification_2_loss: 0.5047 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7575\n",
      "Epoch 95/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7166 - classification_1_loss: 0.2459 - classification_2_loss: 0.4637 - classification_1_acc: 0.8875 - classification_2_acc: 0.7825 - val_loss: 0.7120 - val_classification_1_loss: 0.2106 - val_classification_2_loss: 0.5038 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7575\n",
      "Epoch 96/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7149 - classification_1_loss: 0.2437 - classification_2_loss: 0.4656 - classification_1_acc: 0.8875 - classification_2_acc: 0.7825 - val_loss: 0.7099 - val_classification_1_loss: 0.2101 - val_classification_2_loss: 0.5026 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7600\n",
      "Epoch 97/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7132 - classification_1_loss: 0.2419 - classification_2_loss: 0.4653 - classification_1_acc: 0.8800 - classification_2_acc: 0.7900 - val_loss: 0.7084 - val_classification_1_loss: 0.2083 - val_classification_2_loss: 0.5019 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7625\n",
      "Epoch 98/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7120 - classification_1_loss: 0.2449 - classification_2_loss: 0.4687 - classification_1_acc: 0.8925 - classification_2_acc: 0.7925 - val_loss: 0.7056 - val_classification_1_loss: 0.2074 - val_classification_2_loss: 0.5005 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7625\n",
      "Epoch 99/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7095 - classification_1_loss: 0.2492 - classification_2_loss: 0.4693 - classification_1_acc: 0.8875 - classification_2_acc: 0.7925 - val_loss: 0.7058 - val_classification_1_loss: 0.2081 - val_classification_2_loss: 0.4999 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7600\n",
      "Epoch 100/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.7076 - classification_1_loss: 0.2418 - classification_2_loss: 0.4653 - classification_1_acc: 0.8875 - classification_2_acc: 0.7875 - val_loss: 0.7029 - val_classification_1_loss: 0.2060 - val_classification_2_loss: 0.4988 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 101/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7053 - classification_1_loss: 0.2417 - classification_2_loss: 0.4597 - classification_1_acc: 0.8850 - classification_2_acc: 0.7900 - val_loss: 0.7012 - val_classification_1_loss: 0.2053 - val_classification_2_loss: 0.4977 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7675\n",
      "Epoch 102/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7043 - classification_1_loss: 0.2408 - classification_2_loss: 0.4631 - classification_1_acc: 0.8900 - classification_2_acc: 0.7900 - val_loss: 0.6999 - val_classification_1_loss: 0.2051 - val_classification_2_loss: 0.4970 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7650\n",
      "Epoch 103/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.7013 - classification_1_loss: 0.2413 - classification_2_loss: 0.4600 - classification_1_acc: 0.8950 - classification_2_acc: 0.7925 - val_loss: 0.6974 - val_classification_1_loss: 0.2032 - val_classification_2_loss: 0.4959 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 104/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 0.7007 - classification_1_loss: 0.2497 - classification_2_loss: 0.4586 - classification_1_acc: 0.8900 - classification_2_acc: 0.7925 - val_loss: 0.6957 - val_classification_1_loss: 0.2027 - val_classification_2_loss: 0.4948 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7650\n",
      "Epoch 105/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.6993 - classification_1_loss: 0.2383 - classification_2_loss: 0.4575 - classification_1_acc: 0.8925 - classification_2_acc: 0.7925 - val_loss: 0.6946 - val_classification_1_loss: 0.2015 - val_classification_2_loss: 0.4942 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 106/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.6970 - classification_1_loss: 0.2401 - classification_2_loss: 0.4587 - classification_1_acc: 0.8925 - classification_2_acc: 0.8000 - val_loss: 0.6932 - val_classification_1_loss: 0.2008 - val_classification_2_loss: 0.4935 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7600\n",
      "Epoch 107/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.6958 - classification_1_loss: 0.2348 - classification_2_loss: 0.4631 - classification_1_acc: 0.8925 - classification_2_acc: 0.7950 - val_loss: 0.6914 - val_classification_1_loss: 0.2004 - val_classification_2_loss: 0.4925 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7675\n",
      "Epoch 108/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6941 - classification_1_loss: 0.2346 - classification_2_loss: 0.4533 - classification_1_acc: 0.8950 - classification_2_acc: 0.8000 - val_loss: 0.6912 - val_classification_1_loss: 0.1998 - val_classification_2_loss: 0.4922 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7575\n",
      "Epoch 109/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6920 - classification_1_loss: 0.2390 - classification_2_loss: 0.4538 - classification_1_acc: 0.8950 - classification_2_acc: 0.7975 - val_loss: 0.6887 - val_classification_1_loss: 0.1989 - val_classification_2_loss: 0.4911 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 110/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6911 - classification_1_loss: 0.2367 - classification_2_loss: 0.4525 - classification_1_acc: 0.8950 - classification_2_acc: 0.8000 - val_loss: 0.6877 - val_classification_1_loss: 0.1984 - val_classification_2_loss: 0.4904 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6895 - classification_1_loss: 0.2351 - classification_2_loss: 0.4561 - classification_1_acc: 0.8925 - classification_2_acc: 0.8050 - val_loss: 0.6865 - val_classification_1_loss: 0.1970 - val_classification_2_loss: 0.4900 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7600\n",
      "Epoch 112/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6883 - classification_1_loss: 0.2411 - classification_2_loss: 0.4533 - classification_1_acc: 0.8975 - classification_2_acc: 0.8000 - val_loss: 0.6851 - val_classification_1_loss: 0.1960 - val_classification_2_loss: 0.4894 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7600\n",
      "Epoch 113/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.6864 - classification_1_loss: 0.2318 - classification_2_loss: 0.4476 - classification_1_acc: 0.8950 - classification_2_acc: 0.8000 - val_loss: 0.6839 - val_classification_1_loss: 0.1961 - val_classification_2_loss: 0.4886 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7625\n",
      "Epoch 114/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6850 - classification_1_loss: 0.2359 - classification_2_loss: 0.4507 - classification_1_acc: 0.9000 - classification_2_acc: 0.8075 - val_loss: 0.6831 - val_classification_1_loss: 0.1953 - val_classification_2_loss: 0.4882 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7625\n",
      "Epoch 115/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.6836 - classification_1_loss: 0.2334 - classification_2_loss: 0.4520 - classification_1_acc: 0.8975 - classification_2_acc: 0.8000 - val_loss: 0.6817 - val_classification_1_loss: 0.1951 - val_classification_2_loss: 0.4874 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7650\n",
      "Epoch 116/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.6820 - classification_1_loss: 0.2323 - classification_2_loss: 0.4477 - classification_1_acc: 0.8975 - classification_2_acc: 0.8075 - val_loss: 0.6814 - val_classification_1_loss: 0.1940 - val_classification_2_loss: 0.4873 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 117/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.6811 - classification_1_loss: 0.2302 - classification_2_loss: 0.4565 - classification_1_acc: 0.9000 - classification_2_acc: 0.8075 - val_loss: 0.6803 - val_classification_1_loss: 0.1928 - val_classification_2_loss: 0.4870 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7675\n",
      "Epoch 118/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6792 - classification_1_loss: 0.2322 - classification_2_loss: 0.4520 - classification_1_acc: 0.9000 - classification_2_acc: 0.8100 - val_loss: 0.6782 - val_classification_1_loss: 0.1924 - val_classification_2_loss: 0.4860 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 119/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.6781 - classification_1_loss: 0.2274 - classification_2_loss: 0.4491 - classification_1_acc: 0.9025 - classification_2_acc: 0.8125 - val_loss: 0.6774 - val_classification_1_loss: 0.1925 - val_classification_2_loss: 0.4851 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7675\n",
      "Epoch 120/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.6767 - classification_1_loss: 0.2277 - classification_2_loss: 0.4479 - classification_1_acc: 0.9000 - classification_2_acc: 0.8125 - val_loss: 0.6757 - val_classification_1_loss: 0.1911 - val_classification_2_loss: 0.4846 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 121/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6754 - classification_1_loss: 0.2262 - classification_2_loss: 0.4535 - classification_1_acc: 0.9025 - classification_2_acc: 0.8125 - val_loss: 0.6751 - val_classification_1_loss: 0.1904 - val_classification_2_loss: 0.4843 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 122/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6739 - classification_1_loss: 0.2253 - classification_2_loss: 0.4490 - classification_1_acc: 0.9000 - classification_2_acc: 0.8100 - val_loss: 0.6739 - val_classification_1_loss: 0.1902 - val_classification_2_loss: 0.4836 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 123/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6735 - classification_1_loss: 0.2200 - classification_2_loss: 0.4504 - classification_1_acc: 0.9050 - classification_2_acc: 0.8125 - val_loss: 0.6746 - val_classification_1_loss: 0.1903 - val_classification_2_loss: 0.4837 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7650\n",
      "Epoch 124/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 0.6726 - classification_1_loss: 0.2294 - classification_2_loss: 0.4487 - classification_1_acc: 0.9050 - classification_2_acc: 0.8075 - val_loss: 0.6717 - val_classification_1_loss: 0.1890 - val_classification_2_loss: 0.4824 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7700\n",
      "Epoch 125/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.6703 - classification_1_loss: 0.2212 - classification_2_loss: 0.4489 - classification_1_acc: 0.9075 - classification_2_acc: 0.8100 - val_loss: 0.6709 - val_classification_1_loss: 0.1879 - val_classification_2_loss: 0.4820 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7675\n",
      "Epoch 126/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.6693 - classification_1_loss: 0.2264 - classification_2_loss: 0.4452 - classification_1_acc: 0.9050 - classification_2_acc: 0.8100 - val_loss: 0.6714 - val_classification_1_loss: 0.1875 - val_classification_2_loss: 0.4824 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7650\n",
      "Epoch 127/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6683 - classification_1_loss: 0.2227 - classification_2_loss: 0.4435 - classification_1_acc: 0.9075 - classification_2_acc: 0.8075 - val_loss: 0.6702 - val_classification_1_loss: 0.1867 - val_classification_2_loss: 0.4819 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 128/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.6668 - classification_1_loss: 0.2316 - classification_2_loss: 0.4413 - classification_1_acc: 0.9100 - classification_2_acc: 0.8075 - val_loss: 0.6680 - val_classification_1_loss: 0.1862 - val_classification_2_loss: 0.4806 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 129/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.6671 - classification_1_loss: 0.2303 - classification_2_loss: 0.4443 - classification_1_acc: 0.9075 - classification_2_acc: 0.8100 - val_loss: 0.6679 - val_classification_1_loss: 0.1872 - val_classification_2_loss: 0.4801 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7675\n",
      "Epoch 130/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6649 - classification_1_loss: 0.2241 - classification_2_loss: 0.4518 - classification_1_acc: 0.9075 - classification_2_acc: 0.8100 - val_loss: 0.6659 - val_classification_1_loss: 0.1853 - val_classification_2_loss: 0.4796 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 131/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6642 - classification_1_loss: 0.2191 - classification_2_loss: 0.4431 - classification_1_acc: 0.9050 - classification_2_acc: 0.8100 - val_loss: 0.6664 - val_classification_1_loss: 0.1848 - val_classification_2_loss: 0.4800 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 132/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.6636 - classification_1_loss: 0.2214 - classification_2_loss: 0.4408 - classification_1_acc: 0.9100 - classification_2_acc: 0.8100 - val_loss: 0.6635 - val_classification_1_loss: 0.1844 - val_classification_2_loss: 0.4784 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6619 - classification_1_loss: 0.2189 - classification_2_loss: 0.4349 - classification_1_acc: 0.9075 - classification_2_acc: 0.8100 - val_loss: 0.6640 - val_classification_1_loss: 0.1841 - val_classification_2_loss: 0.4785 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7675\n",
      "Epoch 134/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6610 - classification_1_loss: 0.2181 - classification_2_loss: 0.4396 - classification_1_acc: 0.9075 - classification_2_acc: 0.8100 - val_loss: 0.6630 - val_classification_1_loss: 0.1837 - val_classification_2_loss: 0.4780 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 135/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6599 - classification_1_loss: 0.2174 - classification_2_loss: 0.4344 - classification_1_acc: 0.9125 - classification_2_acc: 0.8125 - val_loss: 0.6630 - val_classification_1_loss: 0.1834 - val_classification_2_loss: 0.4780 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 136/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6586 - classification_1_loss: 0.2160 - classification_2_loss: 0.4389 - classification_1_acc: 0.9125 - classification_2_acc: 0.8100 - val_loss: 0.6613 - val_classification_1_loss: 0.1827 - val_classification_2_loss: 0.4772 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 137/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6584 - classification_1_loss: 0.2142 - classification_2_loss: 0.4393 - classification_1_acc: 0.9100 - classification_2_acc: 0.8075 - val_loss: 0.6610 - val_classification_1_loss: 0.1820 - val_classification_2_loss: 0.4772 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 138/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6571 - classification_1_loss: 0.2184 - classification_2_loss: 0.4422 - classification_1_acc: 0.9100 - classification_2_acc: 0.8075 - val_loss: 0.6597 - val_classification_1_loss: 0.1816 - val_classification_2_loss: 0.4765 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 139/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.6567 - classification_1_loss: 0.2156 - classification_2_loss: 0.4363 - classification_1_acc: 0.9100 - classification_2_acc: 0.8100 - val_loss: 0.6587 - val_classification_1_loss: 0.1811 - val_classification_2_loss: 0.4761 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7700\n",
      "Epoch 140/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6548 - classification_1_loss: 0.2166 - classification_2_loss: 0.4421 - classification_1_acc: 0.9100 - classification_2_acc: 0.8125 - val_loss: 0.6599 - val_classification_1_loss: 0.1809 - val_classification_2_loss: 0.4768 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 141/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.6541 - classification_1_loss: 0.2156 - classification_2_loss: 0.4326 - classification_1_acc: 0.9125 - classification_2_acc: 0.8100 - val_loss: 0.6583 - val_classification_1_loss: 0.1804 - val_classification_2_loss: 0.4759 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 142/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6532 - classification_1_loss: 0.2119 - classification_2_loss: 0.4306 - classification_1_acc: 0.9150 - classification_2_acc: 0.8100 - val_loss: 0.6569 - val_classification_1_loss: 0.1801 - val_classification_2_loss: 0.4752 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7700\n",
      "Epoch 143/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.6523 - classification_1_loss: 0.2191 - classification_2_loss: 0.4412 - classification_1_acc: 0.9100 - classification_2_acc: 0.8100 - val_loss: 0.6559 - val_classification_1_loss: 0.1794 - val_classification_2_loss: 0.4748 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7725\n",
      "Epoch 144/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6525 - classification_1_loss: 0.2140 - classification_2_loss: 0.4352 - classification_1_acc: 0.9125 - classification_2_acc: 0.8100 - val_loss: 0.6548 - val_classification_1_loss: 0.1791 - val_classification_2_loss: 0.4741 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7725\n",
      "Epoch 145/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 0.6517 - classification_1_loss: 0.2110 - classification_2_loss: 0.4338 - classification_1_acc: 0.9150 - classification_2_acc: 0.8125 - val_loss: 0.6574 - val_classification_1_loss: 0.1787 - val_classification_2_loss: 0.4758 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7650\n",
      "Epoch 146/150\n",
      "400/400 [==============================] - 0s 103us/sample - loss: 0.6504 - classification_1_loss: 0.2119 - classification_2_loss: 0.4393 - classification_1_acc: 0.9175 - classification_2_acc: 0.8100 - val_loss: 0.6563 - val_classification_1_loss: 0.1789 - val_classification_2_loss: 0.4750 - val_classification_1_acc: 0.9225 - val_classification_2_acc: 0.7650\n",
      "Epoch 147/150\n",
      "400/400 [==============================] - 0s 112us/sample - loss: 0.6494 - classification_1_loss: 0.2092 - classification_2_loss: 0.4377 - classification_1_acc: 0.9125 - classification_2_acc: 0.8150 - val_loss: 0.6542 - val_classification_1_loss: 0.1781 - val_classification_2_loss: 0.4738 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7700\n",
      "Epoch 148/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.6487 - classification_1_loss: 0.2233 - classification_2_loss: 0.4411 - classification_1_acc: 0.9100 - classification_2_acc: 0.8125 - val_loss: 0.6531 - val_classification_1_loss: 0.1773 - val_classification_2_loss: 0.4733 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7700\n",
      "Epoch 149/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.6476 - classification_1_loss: 0.2183 - classification_2_loss: 0.4352 - classification_1_acc: 0.9150 - classification_2_acc: 0.8100 - val_loss: 0.6537 - val_classification_1_loss: 0.1774 - val_classification_2_loss: 0.4736 - val_classification_1_acc: 0.9175 - val_classification_2_acc: 0.7675\n",
      "Epoch 150/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.6478 - classification_1_loss: 0.2154 - classification_2_loss: 0.4338 - classification_1_acc: 0.9175 - classification_2_acc: 0.8150 - val_loss: 0.6548 - val_classification_1_loss: 0.1770 - val_classification_2_loss: 0.4745 - val_classification_1_acc: 0.9200 - val_classification_2_acc: 0.7650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90bb4b1f50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=150,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "* Does it surprise you that the loss for label 1 is smaller than the loss for label 2?    \n",
    "* Quick side check... do we overtrain?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom Layers\n",
    "\n",
    "Let's define a simple custom layer which is nothing but an affine transformation followed by a relu activation. We need to define:\n",
    "* \\_\\_init\\_\\_()      \n",
    "* build()   \n",
    "* call()   \n",
    "* compute_output_shape()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class MyStupidDenseLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):   \n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super(MyStupidDenseLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_variable(\"W\",\n",
    "                                    shape=[int(input_shape[1]),self.output_dim], \n",
    "                                   initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                   trainable=True)\n",
    "        self.b = self.add_variable(\"b\",\n",
    "                                    shape=[self.output_dim, ], \n",
    "                                   initializer=tf.initializers.zeros(),\n",
    "                                   trainable=True)\n",
    "        super(MyStupidDenseLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.nn.relu(tf.tensordot(x, self.W, axes=[[1],[0]]) + self.b)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our custom layer in the same architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-3858baec01ee>:16: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    }
   ],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model = tf.keras.models.Model(model_input, model_output)\n",
    "model.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 110us/sample - loss: 0.6301 - classification_1_loss: 0.2007 - classification_2_loss: 0.4250 - classification_1_acc: 0.9325 - classification_2_acc: 0.8100 - val_loss: 0.6326 - val_classification_1_loss: 0.1679 - val_classification_2_loss: 0.4642 - val_classification_1_acc: 0.9225 - val_classification_2_acc: 0.7850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90bb005890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=149,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough. So the custom layer works exactly as expected.\n",
    "\n",
    "\n",
    "### 6. Sharing & Freezing Weights \n",
    "\n",
    "\n",
    "Let's define an identical model. The weights for this model will be re-initialized, so upon training for 1 epoch the loss will be much higher than after 150 epochs for the original model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model_mirror = tf.keras.models.Model(model_input, model_output)\n",
    "model_mirror.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 1s 2ms/sample - loss: 1.5032 - classification_1_loss: 0.8021 - classification_2_loss: 0.7035 - classification_1_acc: 0.5000 - classification_2_acc: 0.4725 - val_loss: 1.4658 - val_classification_1_loss: 0.7578 - val_classification_2_loss: 0.6975 - val_classification_1_acc: 0.5000 - val_classification_2_acc: 0.4925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90ba451a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this model would make very different predictions - the loss is ~1.4 vs ~0.6. No surprise, obviously, as this model has not been trained.\n",
    "\n",
    "What if we manually set the the weights of the second model to the weights of the first? Let's look at the layers for both models and then copy the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f90bb15d110>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f90bb15d0d0>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f90bb15d190>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f90bb0d1110>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f90bb165fd0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one layer in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would that be? No magic.. weights and biases of layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.layers[1].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.layers[1].get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. What is the name of the layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct. Is it trainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Are the layers of the second model essentially the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f90bb040a50>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f90bb040a10>,\n",
       " <__main__.MyStupidDenseLayer at 0x7f90bb040ad0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f90bb040fd0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f90bb040f50>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup. (We could look at shapes, etc, but won't do that now.) Now we set the second model's weights to equal the first model's trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mirror.layers[1].set_weights(model.layers[1].get_weights())\n",
    "model_mirror.layers[2].set_weights(model.layers[2].get_weights())\n",
    "model_mirror.layers[3].set_weights(model.layers[3].get_weights())\n",
    "model_mirror.layers[4].set_weights(model.layers[4].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why don't we set the weights for layer 0?\n",
    "\n",
    "Okay. What is the loss now for the mirror model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 110us/sample - loss: 0.6311 - classification_1_loss: 0.2038 - classification_2_loss: 0.4212 - classification_1_acc: 0.9250 - classification_2_acc: 0.8125 - val_loss: 0.6334 - val_classification_1_loss: 0.1688 - val_classification_2_loss: 0.4639 - val_classification_1_acc: 0.9250 - val_classification_2_acc: 0.7850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90ba1fa690>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Essentially the same as for the original one! (Not a surprise actually... but great that it works.)\n",
    "\n",
    "Let's now turn to **freezing** layers. For example, you may want to copy weights from another model but hold those weights fixed upon further training. To test this, we will do a simple toy exercise: freeze a layer of the second model and train it further. Let's compare some weights before and after training. \n",
    "\n",
    "We start by defining a new model that again has the same architecture, **but we set the trainable-parameter for one layer to False**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "dense_layer_2.trainable = False                                   # Freeze this layer\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model_mirror_2 = tf.keras.models.Model(model_input, model_output)\n",
    "model_mirror_2.compile(loss=losses,  optimizer='adam', metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we again set the weights to mirror the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mirror_2.layers[1].set_weights(model.layers[1].get_weights())\n",
    "model_mirror_2.layers[2].set_weights(model.layers[2].get_weights())\n",
    "model_mirror_2.layers[3].set_weights(model.layers[3].get_weights())\n",
    "model_mirror_2.layers[4].set_weights(model.layers[4].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are (some of) the weights now, before we train further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16306525,  0.3430157 , -0.13253239],\n",
       "       [-0.1442931 ,  0.7501852 , -0.06985269],\n",
       "       [-0.4240862 ,  0.58976454, -0.3947481 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58394676,  0.05053842, -0.35226533],\n",
       "       [-0.01004035,  0.3307187 , -0.03672026],\n",
       "       [-0.2914849 , -0.14890574, -0.67025244]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare that with the values post further training. Let's also check the 'trainable' settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so it shows that one hidden layer supposedly is trainable, the other one is not. Is that true? We train more and compare weights compared to what they were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 110us/sample - loss: 0.6161 - classification_1_loss: 0.1892 - classification_2_loss: 0.4205 - classification_1_acc: 0.9275 - classification_2_acc: 0.8125 - val_loss: 0.6260 - val_classification_1_loss: 0.1608 - val_classification_2_loss: 0.4639 - val_classification_1_acc: 0.9275 - val_classification_2_acc: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90ba02ee90>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=40,\n",
    "    verbose=0\n",
    ")\n",
    "model_mirror_2.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20854098,  0.35762998, -0.13515098],\n",
       "       [-0.16344354,  0.7268433 , -0.046384  ],\n",
       "       [-0.4516486 ,  0.61464345, -0.4081515 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights **did change**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58394676,  0.05053842, -0.35226533],\n",
       "       [-0.01004035,  0.3307187 , -0.03672026],\n",
       "       [-0.2914849 , -0.14890574, -0.67025244]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights **did not change**. So here we could see layer freezing at work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
